{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a dataset for binary sentiment classification. It provides a set of 50,000 polar movie reviews for training and testing.\n",
    "\n",
    "Firstly, downlaod and import everything that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load data from cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"https://drive.google.com/uc?export=download&id=16CHSMU1ffqMZc__bTZngjZOjREghWKid\"\n",
    "test_data_path = \"https://drive.google.com/uc?export=download&id=1tUMuS0ol19IUO8zgOcHpHWLV2pQVA8qZ\"\n",
    "\n",
    "# Get data locally during developing to avoid fetching time. (!) Comment when deploying and uncomment upper two\n",
    "# train_data_path = \"../work_data/train.csv\"\n",
    "# test_data_path = \"../work_data/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check our dataset for imbalance so we know are any additional steps required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='count', ylabel='sentiment'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train_data[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, luckily for is, the dataset is ideally balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[\"review\"]\n",
    "y_train = train_data[\"sentiment\"]\n",
    "\n",
    "X_test = test_data[\"review\"]\n",
    "y_test = test_data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, i will remove any redundant characters, that doesnt provide any value for sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i caught this little gem totally by accident b...\n",
       "1    i can believe that let myself into this movie ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', str(text))  # Remove special characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # Remove single characters\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)  # Remove single characters from start\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)  # Replace multiple spaces with single space\n",
    "    text = re.sub(r'^b\\s+', '', text)  # Remove prefixed 'b'\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "X_train = X_train.apply(clean_text)\n",
    "X_test = X_test.apply(clean_text)\n",
    "\n",
    "X_train.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will perform tokenization. In this case, I decided that making each word separate token is good enough for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, caught, this, little, gem, totally, by, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, can, believe, that, let, myself, into, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[spoiler, alert, it, just, gets, to, me, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[if, there, one, thing, ve, learnt, from, watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, remember, when, this, was, in, theaters, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  [i, caught, this, little, gem, totally, by, ac...\n",
       "1  [i, can, believe, that, let, myself, into, thi...\n",
       "2  [spoiler, alert, it, just, gets, to, me, the, ...\n",
       "3  [if, there, one, thing, ve, learnt, from, watc...\n",
       "4  [i, remember, when, this, was, in, theaters, r..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "X_train = X_train.apply(tokenize)\n",
    "X_test = X_test.apply(tokenize)\n",
    "\n",
    "X_train.to_frame().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-words filtering\n",
    "\n",
    "Now, removing stopwords. Applies the same logic as in removing characters: removing that ones, which will not provide any value for sentiment analysis, but now we will use predefined downloaded set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [caught, little, gem, totally, accident, back,...\n",
       "1    [believe, let, movie, accomplish, favor, frien...\n",
       "2    [spoiler, alert, gets, nerve, people, remake, ...\n",
       "3    [one, thing, learnt, watching, george, romero,...\n",
       "4    [remember, theaters, reviews, said, horrible, ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in STOPWORDS]\n",
    "\n",
    "X_train = X_train.apply(remove_stopwords)\n",
    "X_test = X_test.apply(remove_stopwords)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seen on example that it removed words like \"I\", \"this\", \"by\" and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vs Lemmatization\n",
    "\n",
    "Stemming and lemmatization are both techniques used to reduce words to their base or root form.\n",
    "\n",
    "Stemming is a rule-based process that removes suffixes to get the root form of a word, often leading to non-real words.\n",
    "\n",
    "Lemmatization reduces words to their dictionary form (lemma) using linguistic knowledge, ensuring real words.\n",
    "\n",
    "To decide which one to use, I need to try both:\n",
    "\n",
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [caught, littl, gem, total, accid, back, 1980,...\n",
       "1    [believ, let, movi, accomplish, favor, friend,...\n",
       "2    [spoiler, alert, get, nerv, peopl, remak, use,...\n",
       "3    [one, thing, learnt, watch, georg, romero, cre...\n",
       "4    [rememb, theater, review, said, horribl, well,...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return [stemmer.stem(word) for word in text]\n",
    "\n",
    "X_train_stemmed = X_train.apply(stem_words)\n",
    "\n",
    "X_train_stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [caught, little, gem, totally, accident, back,...\n",
       "1    [believe, let, movie, accomplish, favor, frien...\n",
       "2    [spoiler, alert, get, nerve, people, remake, u...\n",
       "3    [one, thing, learnt, watching, george, romero,...\n",
       "4    [remember, theater, review, said, horrible, we...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "X_train_lemmatized = X_train.apply(lemmatize_words)\n",
    "\n",
    "X_train_lemmatized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, conclusions. While lemmatization has left some wordds unchanged (like caught, learnt), it provided much more accurate results, while stemming often made not existing words (which is really important considering I will later try word embedding), and at the same time, lemmatization was ~3 times faster for me. That leaves no room for any reason to not prefer lemmatization over stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_lemmatized\n",
    "\n",
    "X_test = X_test.apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Machine learning models cant work with strings: they firstly need to be converted to some numerical form. I will try to different methods of vectorization: Bag-of-Words and Word Embedding\n",
    "\n",
    "### Bag-of-Words\n",
    "\n",
    "-A simple, frequency-based text representation where each word is treated as an independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train = X_train.apply(lambda words: ' '.join(words))  # Convert lists to strings\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.apply(lambda words: ' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "Word embedding is on the other hand, a dense vector representation of words that captures semantic meaning and relationships, that positively distinguishes this method from the previous one. While its pros are obvious, the cons are that it requires more computation, and needs either big dataset to train effectively or pretrained models. In this case, i will use one called Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")\n",
    "# word2vec = KeyedVectors.load(\"word2vec-google-news-300.kv\", mmap='r')\n",
    "\n",
    "# Little test to ensure its working\n",
    "vector = word2vec[\"king\"]\n",
    "similarity = word2vec.similarity(\"king\", \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510957"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec.save(\"word2vec-google-news-300.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions: for sentiment analysis, Word embediing is much, much more suitable choice, so I will go on with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Now that we applied vectorization to datasets, usual clssification models can be used to do sentiment nalysys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(tokenize)\n",
    "X_test = X_test.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [caught, little, gem, totally, accident, back,...\n",
       "1        [believe, let, movie, accomplish, favor, frien...\n",
       "2        [spoiler, alert, get, nerve, people, remake, u...\n",
       "3        [one, thing, learnt, watching, george, romero,...\n",
       "4        [remember, theater, review, said, horrible, we...\n",
       "                               ...                        \n",
       "39995    [1920s, man, named, walt, disney, mission, sat...\n",
       "39996    [first, time, saw, shade, sneakpreview, even, ...\n",
       "39997    [waste, time, danger, watch, tempted, tear, dv...\n",
       "39998    [far, pathetic, movie, indian, cinema, cinema,...\n",
       "39999    [movie, forever, left, impression, watched, fr...\n",
       "Name: review, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, model, num_features=300):\n",
    "    # Filter tokens that exist in the Word2Vec vocabulary\n",
    "    tokens = [word for word in tokens_list if word in model.key_to_index]\n",
    "    \n",
    "    if len(tokens) == 0:  # If no valid words in the model, return a zero vector\n",
    "        return np.zeros(num_features)\n",
    "    \n",
    "    # Calculate the average of word vectors for the tokens\n",
    "    word_vectors = [model[word] for word in tokens]\n",
    "    avg_vector = np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    return avg_vector\n",
    "\n",
    "# Apply to your tokenized reviews\n",
    "X_train = np.array([get_average_word2vec(review, word2vec) for review in X_train])\n",
    "\n",
    "\n",
    "X_test = np.array([get_average_word2vec(review, word2vec) for review in X_test])\n",
    "\n",
    "# Now, X_train_word2vec is a 2D array with each row representing the average Word2Vec vector for each review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06581443,  0.01023283,  0.0313708 , ..., -0.06038393,\n",
       "         0.01935032,  0.00904265],\n",
       "       [ 0.07401692,  0.02514042,  0.00329694, ..., -0.05986691,\n",
       "         0.04118495, -0.00026885],\n",
       "       [ 0.05933053, -0.01295399,  0.03067889, ..., -0.06609236,\n",
       "        -0.00385938, -0.01924606],\n",
       "       ...,\n",
       "       [ 0.06910837,  0.06125518,  0.01687556, ..., -0.09493661,\n",
       "        -0.02089013,  0.00164983],\n",
       "       [ 0.07151088,  0.01659382,  0.01234732, ..., -0.0511319 ,\n",
       "         0.00589957,  0.00461692],\n",
       "       [ 0.06243185,  0.02599386,  0.01262385, ..., -0.04069519,\n",
       "         0.03703458,  0.00198275]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.apply(lambda x: 1 if x == 'positive' else 0)\n",
    "y_test = y_test.apply(lambda x: 1 if x == 'positive' else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 300)\n",
      "y_train shape: (40000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Check the shape before fitting\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (num_reviews, 300)\n",
    "print(\"y_train shape:\", y_train.shape)          # Should be (num_reviews,)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "model.fit(X_train, y_train)  # No need to convert y_train to array\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(random_state=SEED)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghj\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6458 - loss: 0.6070 - val_accuracy: 0.8398 - val_loss: 0.3667\n",
      "Epoch 2/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8353 - loss: 0.3969 - val_accuracy: 0.8465 - val_loss: 0.3489\n",
      "Epoch 3/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8433 - loss: 0.3735 - val_accuracy: 0.8456 - val_loss: 0.3470\n",
      "Epoch 4/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8492 - loss: 0.3630 - val_accuracy: 0.8501 - val_loss: 0.3430\n",
      "Epoch 5/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8491 - loss: 0.3592 - val_accuracy: 0.8572 - val_loss: 0.3324\n",
      "Epoch 6/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8526 - loss: 0.3536 - val_accuracy: 0.8509 - val_loss: 0.3341\n",
      "Epoch 7/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8522 - loss: 0.3469 - val_accuracy: 0.8557 - val_loss: 0.3310\n",
      "Epoch 8/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8541 - loss: 0.3485 - val_accuracy: 0.8502 - val_loss: 0.3348\n",
      "Epoch 9/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8582 - loss: 0.3447 - val_accuracy: 0.8557 - val_loss: 0.3347\n",
      "Epoch 10/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8596 - loss: 0.3417 - val_accuracy: 0.8615 - val_loss: 0.3252\n",
      "Epoch 11/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8595 - loss: 0.3349 - val_accuracy: 0.8582 - val_loss: 0.3250\n",
      "Epoch 12/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8598 - loss: 0.3360 - val_accuracy: 0.8599 - val_loss: 0.3251\n",
      "Epoch 13/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8611 - loss: 0.3312 - val_accuracy: 0.8568 - val_loss: 0.3295\n",
      "Epoch 14/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8597 - loss: 0.3364 - val_accuracy: 0.8644 - val_loss: 0.3219\n",
      "Epoch 15/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8629 - loss: 0.3281 - val_accuracy: 0.8621 - val_loss: 0.3213\n",
      "Epoch 16/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8639 - loss: 0.3265 - val_accuracy: 0.8591 - val_loss: 0.3268\n",
      "Epoch 17/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8660 - loss: 0.3225 - val_accuracy: 0.8589 - val_loss: 0.3291\n",
      "Epoch 18/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8684 - loss: 0.3181 - val_accuracy: 0.8597 - val_loss: 0.3290\n",
      "Epoch 19/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8668 - loss: 0.3209 - val_accuracy: 0.8589 - val_loss: 0.3279\n",
      "Epoch 20/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8677 - loss: 0.3204 - val_accuracy: 0.8624 - val_loss: 0.3264\n",
      "Epoch 21/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8692 - loss: 0.3172 - val_accuracy: 0.8605 - val_loss: 0.3268\n",
      "Epoch 22/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8701 - loss: 0.3143 - val_accuracy: 0.8601 - val_loss: 0.3286\n",
      "Epoch 23/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8704 - loss: 0.3123 - val_accuracy: 0.8608 - val_loss: 0.3283\n",
      "Epoch 24/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8691 - loss: 0.3143 - val_accuracy: 0.8580 - val_loss: 0.3267\n",
      "Epoch 25/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8731 - loss: 0.3090 - val_accuracy: 0.8557 - val_loss: 0.3339\n",
      "Epoch 26/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8711 - loss: 0.3067 - val_accuracy: 0.8581 - val_loss: 0.3313\n",
      "Epoch 27/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8754 - loss: 0.3029 - val_accuracy: 0.8640 - val_loss: 0.3209\n",
      "Epoch 28/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8749 - loss: 0.3048 - val_accuracy: 0.8628 - val_loss: 0.3202\n",
      "Epoch 29/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.3011 - val_accuracy: 0.8629 - val_loss: 0.3242\n",
      "Epoch 30/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8711 - loss: 0.3087 - val_accuracy: 0.8563 - val_loss: 0.3311\n",
      "Epoch 31/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8755 - loss: 0.2997 - val_accuracy: 0.8653 - val_loss: 0.3197\n",
      "Epoch 32/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8776 - loss: 0.2954 - val_accuracy: 0.8658 - val_loss: 0.3210\n",
      "Epoch 33/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8774 - loss: 0.2972 - val_accuracy: 0.8619 - val_loss: 0.3285\n",
      "Epoch 34/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8794 - loss: 0.2985 - val_accuracy: 0.8634 - val_loss: 0.3268\n",
      "Epoch 35/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8795 - loss: 0.2964 - val_accuracy: 0.8654 - val_loss: 0.3235\n",
      "Epoch 36/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8813 - loss: 0.2934 - val_accuracy: 0.8647 - val_loss: 0.3222\n",
      "Epoch 37/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8791 - loss: 0.2887 - val_accuracy: 0.8615 - val_loss: 0.3292\n",
      "Epoch 38/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8802 - loss: 0.2891 - val_accuracy: 0.8637 - val_loss: 0.3257\n",
      "Epoch 39/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8813 - loss: 0.2891 - val_accuracy: 0.8620 - val_loss: 0.3275\n",
      "Epoch 40/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8840 - loss: 0.2848 - val_accuracy: 0.8622 - val_loss: 0.3249\n",
      "Epoch 41/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8798 - loss: 0.2877 - val_accuracy: 0.8629 - val_loss: 0.3336\n",
      "Epoch 42/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8838 - loss: 0.2814 - val_accuracy: 0.8589 - val_loss: 0.3334\n",
      "Epoch 43/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8798 - loss: 0.2883 - val_accuracy: 0.8626 - val_loss: 0.3272\n",
      "Epoch 44/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8828 - loss: 0.2858 - val_accuracy: 0.8653 - val_loss: 0.3249\n",
      "Epoch 45/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8828 - loss: 0.2825 - val_accuracy: 0.8636 - val_loss: 0.3299\n",
      "Epoch 46/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8853 - loss: 0.2813 - val_accuracy: 0.8643 - val_loss: 0.3270\n",
      "Epoch 47/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8859 - loss: 0.2756 - val_accuracy: 0.8654 - val_loss: 0.3282\n",
      "Epoch 48/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8857 - loss: 0.2760 - val_accuracy: 0.8580 - val_loss: 0.3296\n",
      "Epoch 49/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8833 - loss: 0.2822 - val_accuracy: 0.8621 - val_loss: 0.3295\n",
      "Epoch 50/50\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8858 - loss: 0.2778 - val_accuracy: 0.8665 - val_loss: 0.3262\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step\n",
      "Keras Model Accuracy: 0.8665\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model_keras = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_keras.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_keras.fit(X_train, y_train, epochs=50, batch_size=256, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions\n",
    "y_pred_keras = model_keras.predict(X_test)\n",
    "y_pred_keras_classes = (y_pred_keras > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = (y_pred_keras_classes.flatten() == y_test).mean()\n",
    "print(f\"Keras Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my final project (MLE part) I will choose Logistic Regressions. It's simple, has a accuracy score not much less than DL model or SVM (0.8554 against ~0.86), and extremely fast.\n",
    "\n",
    "Of course, my results aren't 100% accurate. I could have played more with hyperparameters tuning, try more DL model architectures (I actually did it but left it all behind in final notebook version). I could have tried more vectorization methods, like TF-IDF. But at the end, chosen model completes our task — sentiment analysis with accuracy > 0.85, so I will b satisfied for now with this result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
